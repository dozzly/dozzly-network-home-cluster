#cloud-config
manage_etc_hosts: true
ssh_pwauth: false

# --- 0) Early: prepare dnsmasq iface binding (no duplicates later) ---
bootcmd:
  - [ bash, -lc, 'set -euo pipefail;
      # Block auto-start of services during package install
      cat >/usr/sbin/policy-rc.d <<EOF
      #!/bin/sh
      # 101 = "action not allowed"
      exit 101
      EOF
      chmod 0755 /usr/sbin/policy-rc.d;

      # Prepare dnsmasq interface file early to avoid races
      mkdir -p /etc/dnsmasq.d;
      IFACE="$(ip -o -4 route show to default | awk "{print \$5}")";
      printf "%s\n" "bind-interfaces" "interface=${IFACE}" "except-interface=lo" > /etc/dnsmasq.d/00-iface.conf' ]


# --- 1) Users ---
users:
  - name: ${admin_user}
    sudo: ALL=(ALL) NOPASSWD:ALL
    shell: /bin/bash
    lock_passwd: false
    # Optional console password (hash must be SHA-512: starts with $6$…)
    passwd: ${ssh_password_hash}
    ssh_authorized_keys:
      - ${ssh_pubkey}

  # System account for matchbox
  - name: matchbox
    system: true
    shell: /usr/sbin/nologin
    homedir: /srv/matchbox

# --- 2) Packages we’ll need ---
package_update: true
package_upgrade: true
packages:
  - git
  - curl
  - jq
  - dnsmasq
  - tftpd-hpa
  - ca-certificates
  - wget
  - unzip
  - apt-transport-https
  - lsb-release
  - gnupg
  - ufw
  - ca-certificates

# --- 3) Files (scripts, units, configs) ---
write_files:

  - path: /etc/systemd/system/systemd-networkd-wait-online.service.d/override.conf
    permissions: "0644"
    owner: "root:root"
    content: |
      [Service]
      ExecStart=
      ExecStart=/lib/systemd/systemd-networkd-wait-online --any --timeout=15

  # Admin bootstrap (logs to /var/log/admin-setup.log)
  - path: /usr/local/sbin/admin-setup.sh
    permissions: "0755"
    owner: "root:root"
    content: |
      #!/usr/bin/env bash
      set -u
      exec > >(tee -a /var/log/admin-setup.log) 2>&1
      echo "== $(date -Is) start =="
      export DEBIAN_FRONTEND=noninteractive
      apt-get update || true
      apt-get install -y -o Dpkg::Options::="--force-confold" caddy || true
      systemctl enable --now caddy || true
      echo "== $(date -Is) done =="

  # Script that (re)writes the iface binding file
  - path: /usr/local/sbin/gen-dnsmasq-iface.sh
    permissions: "0755"
    owner: "root:root"
    content: |
      #!/usr/bin/env bash
      set -euo pipefail
      IFACE="$(ip -o -4 route show to default | awk '{print $5}')"
      [ -n "${IFACE:-}" ] || { echo "No default-route iface found"; exit 1; }
      mkdir -p /etc/dnsmasq.d
      {
        echo "bind-interfaces"
        echo "interface=${IFACE}"
        echo "except-interface=lo"
      } > /etc/dnsmasq.d/00-iface.conf

  # Run the generator before dnsmasq
  - path: /etc/systemd/system/dnsmasq-iface.service
    permissions: "0644"
    owner: "root:root"
    content: |
      [Unit]
      Description=Generate dnsmasq interface configuration
      After=network-pre.target
      Wants=network-pre.target

      [Service]
      Type=oneshot
      ExecStart=/usr/local/sbin/gen-dnsmasq-iface.sh
      RemainAfterExit=yes

      [Install]
      WantedBy=multi-user.target

  # dnsmasq: wait until iface file exists and network is online
  - path: /etc/systemd/system/dnsmasq.service.d/10-cloudinit.conf
    permissions: "0644"
    owner: "root:root"
    content: |
      [Unit]
      After=network-online.target dnsmasq-iface.service
      Wants=network-online.target dnsmasq-iface.service

      [Service]
      # Optional: wait until the iface has IPv4
      ExecStartPre=/bin/sh -c 'IFACE=$(sed -n "s/^interface=//p" /etc/dnsmasq.d/00-iface.conf); \
        for i in $(seq 1 30); do ip -4 addr show dev "$IFACE" | grep -q "inet " && exit 0; sleep 1; done; \
        echo "Interface $IFACE has no IPv4 addr; refusing to start dnsmasq" >&2; exit 1'

  # setup tftpd-hpa
  - path: /etc/default/tftpd-hpa
    permissions: "0644"
    owner: "root:root"
    content: |
      TFTP_USERNAME="tftp"
      TFTP_DIRECTORY="/var/lib/tftpboot"
      TFTP_ADDRESS=":69"
      TFTP_OPTIONS="--secure --create"

  - path: /usr/local/sbin/debug-pxe.sh
    permissions: "0755"
    owner: "root:root"
    content: |
      #!/usr/bin/env bash
      set -e
      echo "== networkd-wait-online =="; systemctl status --no-pager systemd-networkd-wait-online || true
      echo "== iface conf =="; cat /etc/dnsmasq.d/00-iface.conf || true
      echo "== dnsmasq test =="; dnsmasq --test || true
      echo "== dnsmasq logs =="; journalctl -u dnsmasq -b --no-pager | tail -n 80
      echo "== matchbox logs =="; journalctl -u matchbox -b --no-pager | tail -n 120

  # dnsmasq minimal ProxyDHCP config (NO bind-interfaces here; we set it via 00-iface.conf)
  - path: /usr/local/sbin/setup-pxe.sh
    permissions: "0755"
    owner: "root:root"
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Load admin config
      if [[ -f /etc/cloud/admin.env ]]; then
        # shellcheck disable=SC1091
        . /etc/cloud/admin.env
      fi

      # --- Required variables (from /etc/cloud/admin.env) ---
      ADMIN_VM_IP="${ADMIN_VM_IP:-}"
      NUC1_MAC="${NUC1_MAC:-}"
      NUC2_MAC="${NUC2_MAC:-}"
      : "${ADMIN_VM_IP:?Set ADMIN_VM_IP in /etc/cloud/admin.env}"

      echo "[setup-pxe] using ADMIN_VM_IP=${ADMIN_VM_IP}"

      export DEBIAN_FRONTEND=noninteractive
      apt-get update -y
      apt-get install -y --no-install-recommends dnsmasq tftpd-hpa curl ca-certificates \
        build-essential git liblzma-dev mtools xz-utils uuid-dev file jq

      # ---------- TFTP layout ----------
      TFTP_ROOT=/var/lib/tftpboot
      IPXE_DIR="$TFTP_ROOT/ipxe"
      install -d -m 0755 "$IPXE_DIR"

      # x86 BIOS PXE (optional; ignore failure)
      curl --fail --location -fsS --retry 3 -o "$IPXE_DIR/undionly.kpxe" https://boot.ipxe.org/undionly.kpxe || true

      # x86_64 UEFI iPXE (prebuilt)
      curl --fail --location -fsS --retry 3 -o "$IPXE_DIR/ipxe_x86_64.efi" https://boot.ipxe.org/ipxe.efi

      # ARM64 UEFI iPXE (prebuilt)
      curl --fail --location -fsS --retry 3 -o "$IPXE_DIR/snponly_arm64.efi" https://boot.ipxe.org/arm64-efi/snponly.efi
      curl --fail --location -fsS --retry 3 -o "$IPXE_DIR/ipxe_arm64.efi"    https://boot.ipxe.org/arm64-efi/ipxe.efi

      # Set ownership/perms after downloads
      chown -R tftp:tftp "$TFTP_ROOT"
      chmod 0644 "$IPXE_DIR"/ipxe_x86_64.efi "$IPXE_DIR"/snponly_arm64.efi "$IPXE_DIR"/ipxe_arm64.efi 2>/dev/null || true
      chmod 0644 "$IPXE_DIR"/undionly.kpxe 2>/dev/null || true

      # --- Sanity: confirm architectures (case-insensitive, robust) ---
      x86_info="$(file -b "$IPXE_DIR/ipxe_x86_64.efi" || true)"
      arm_info_snponly="$(file -b "$IPXE_DIR/snponly_arm64.efi" || true)"
      arm_info_ipxe="$(file -b "$IPXE_DIR/ipxe_arm64.efi" || true)"

      echo "[ipxe] x86_64: $x86_info"
      echo "[ipxe] arm64 snp: $arm_info_snponly"
      echo "[ipxe] arm64 ipxe: $arm_info_ipxe"

      echo "$x86_info" | grep -qiE 'x86-64|x86_64|PE32\+ executable.*EFI' \
        || { echo "ipxe_x86_64.efi not x86-64?"; exit 1; }

      echo "$arm_info_snponly" | grep -qiE 'aarch64|arm64' \
        || { echo "snponly_arm64.efi not aarch64?"; exit 1; }

      echo "$arm_info_ipxe" | grep -qiE 'aarch64|arm64' \
        || { echo "ipxe_arm64.efi not aarch64?"; exit 1; }
      # ---------------------------------------------------------------

      # ---------- dnsmasq ProxyDHCP ----------
      cat >/etc/dnsmasq.d/pxe.conf <<EOF
      # ProxyDHCP only
      port=0
      log-dhcp

      # No leases here; we're just the PXE proxy
      dhcp-range=192.168.1.0,proxy

      # Arch tags (DHCP option 93)
      dhcp-match=set:bios,option:client-arch,0
      dhcp-match=set:efi64,option:client-arch,7
      dhcp-match=set:efiarm64,option:client-arch,11

      # Detect iPXE (user-class 77)
      dhcp-userclass=set:ipxe,iPXE

      # iPXE clients -> matchbox
      dhcp-boot=net:ipxe,http://${ADMIN_VM_IP}:8080/boot.ipxe?mac=\${net0/mac}

      # First-stage binaries
      dhcp-boot=tag:bios,ipxe/undionly.kpxe,,${ADMIN_VM_IP}
      dhcp-boot=tag:efi64,ipxe/ipxe_x86_64.efi,,${ADMIN_VM_IP}
      dhcp-boot=tag:efiarm64,ipxe/snponly_arm64.efi,,${ADMIN_VM_IP}
      EOF

      # Ensure tftpd-hpa serves our TFTP root
      if grep -q '^TFTP_DIRECTORY=' /etc/default/tftpd-hpa 2>/dev/null; then
        sed -i "s|^TFTP_DIRECTORY=.*|TFTP_DIRECTORY=\"$TFTP_ROOT\"|g" /etc/default/tftpd-hpa
      else
        cat >/etc/default/tftpd-hpa <<EOD
      TFTP_USERNAME="tftp"
      TFTP_DIRECTORY="$TFTP_ROOT"
      TFTP_ADDRESS="0.0.0.0:69"
      TFTP_OPTIONS="--secure"
      EOD
      fi

      # ---------- restart services ----------
      systemctl daemon-reload
      systemctl enable --now tftpd-hpa
      systemctl restart tftpd-hpa
      systemctl enable --now dnsmasq
      systemctl restart dnsmasq

      echo "[setup-pxe] ready: ProxyDHCP (dnsmasq) + TFTP ($TFTP_ROOT) + iPXE (arm64 + x86_64)."

  # Simple iPXE script (example: Talos)
  - path: /srv/matchbox/pxe/talos.ipxe
    permissions: "0644"
    owner: "root:root"
    content: |
      #!ipxe
      set base-url http://${admin_server_ip}/talos
      kernel ${base-url}/vmlinuz-initrd
      initrd ${base-url}/initramfs.img
      imgargs vmlinuz-initrd talos.config=http://${admin_server_ip}/configs/${net0/mac}.yaml
      boot

  # Matchbox systemd unit
  - path: /etc/systemd/system/matchbox.service
    permissions: "0644"
    owner: "root:root"
    content: |
      [Unit]
      Description=Matchbox PXE profile server
      After=network-online.target
      Wants=network-online.target

      [Service]
      User=matchbox
      Group=matchbox
      WorkingDirectory=/srv/matchbox
      ExecStartPre=/usr/bin/test -x /usr/local/bin/matchbox
      ExecStartPre=/usr/bin/test -d /srv/matchbox
      ExecStartPre=/usr/bin/test -d /srv/matchbox/assets
      ExecStart=/usr/local/bin/matchbox \
        -address=0.0.0.0:8080 \
        -rpc-address=0.0.0.0:8081 \
        -data-path=/srv/matchbox \
        -assets-path=/srv/matchbox/assets \
        -log-level=debug
      Restart=on-failure
      RestartSec=2s
      StandardOutput=journal
      StandardError=journal
      # Hardening (optional)
      NoNewPrivileges=true
      ProtectHome=true
      ProtectSystem=full

      [Install]
      WantedBy=multi-user.target

  # Caddyfile to serve static assets
  - path: /etc/caddy/Caddyfile
    permissions: "0644"
    owner: "root:root"
    content: |
      :80 {
        root * /srv/matchbox
        file_server browse
      }

  # Krew installer (parameterized for ${admin_user})
  - path: /usr/local/bin/krew-install.sh
    permissions: "0755"
    owner: "root:root"
    content: |
      #!/usr/bin/env bash
      set -euxo pipefail
      id -u "${admin_user}" >/dev/null 2>&1 || useradd -m -s /bin/bash "${admin_user}"
      sudo -u "${admin_user}" bash -lc '
        set -euo pipefail
        TMP="$(mktemp -d)"
        cd "$TMP"
        OS="$(uname | tr "[:upper:]" "[:lower:]")"
        ARCH="$(uname -m | sed -e "s/x86_64/amd64/" -e "s/aarch64/arm64/")"
        curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/krew-${OS}_${ARCH}.tar.gz"
        tar --warning=no-unknown-keyword -xzf "krew-${OS}_${ARCH}.tar.gz" 2>/dev/null || tar -xzf "krew-${OS}_${ARCH}.tar.gz"
        "./krew-${OS}_${ARCH}" install krew
        grep -q ".krew/bin" ~/.bashrc || echo "export PATH=\$PATH:\$HOME/.krew/bin" >> ~/.bashrc
        export PATH="$HOME/.krew/bin:$PATH"
        kubectl krew install ctx ns df-pv neat view-secret node-shell
        kubectl krew upgrade
      '

  # Helm installer
  - path: /usr/local/bin/helm-install.sh
    permissions: "0755"
    owner: "root:root"
    content: |
      #!/usr/bin/env bash
      set -euxo pipefail
      curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

  # -------- editable config (fill in later; safe defaults) --------
  - path: /etc/cloud/admin.env
    permissions: "0644"
    owner: root:root
    content: |
      # ===== Admin VM config (fill as you learn details) =====
      # External backup disk
      BACKUP_UUID=""            # e.g. output of: blkid
      BACKUP_FSTYPE="ext4"      # ext4/xfs/btrfs
      BACKUP_MOUNT="/mnt/backup"

      # /etc/cloud/admin.env (append these)
      CLUSTER_NAME="dozzly-cluster"
      CLUSTER_DOMAIN="downhill.dozzly.network"
      CREATE_FLUX_SECRET=yes

      # VIPs / host IPs
      VIP_INGRESS_IP="192.168.1.150"
      VIP_API_IP="192.168.1.151"

      ADMIN_VM_HOSTNAME="atherton"
      ADMIN_VM_IP="192.168.1.152"

      CONTROL_VM_IP="192.168.1.155"
      CONTROL_VM_HOSTNAME="warner"
      CONTROL_VM_IFACE=enp1s0
      CONTROL_VM_MAC="AA:BB:CC:DD:EE:03"
      CONTROL_VM_ROLE=control

      NUC1_HOSTNAME="goldstone"
      NUC1_IP="192.168.1.153"
      NUC1_IFACE=enp1s0
      NUC1_MAC=AA:BB:CC:DD:EE:01
      NUC1_ROLE=control

      NUC2_HOSTNAME="gee"
      NUC2_IP="192.168.1.154"
      NUC2_MAC=AA:BB:CC:DD:EE:02
      NUC2_ROLE=control
      NUC2_IFACE=enp1s0

      GATEWAY=192.168.1.1
      DNS1=1.1.1.1
      DNS2=8.8.8.8

      # Where to stash Talos outputs
      TALOS_OUT_DIR="/srv/talos"
      TALOS_VERSION="v1.11.1"
      TALOS_FORCE_REGEN="no"

      # MinIO creds (change later/rotate as needed)
      MINIO_ROOT_USER="minioadmin"
      MINIO_ROOT_PASSWORD="change-me-please"
      MINIO_HOSTNAME="admin-server.lan"
      MINIO_PORT="9000"
      MINIO_CONSOLE_PORT="9001"
      MINIO_BUCKET_LONGHORN="longhorn-backups"
      MINIO_BUCKET_VMS="vm-backups"

      # GitOps repo (Flux will read from this)
      GITOPS_DIR="/srv/git/dozzly-network-home-cluster"
      GIT_REMOTE="git@github.com:dozzly/dozzly-network-home-cluster.git"
      GIT_BRANCH="main"
      ADMIN_GIT_NAME="dozzly"
      ADMIN_GIT_EMAIL="jack.child@gmail.com"

      # Local admin user who should own SSH keys
      ADMIN_USER="${USER}"

  # -------- flux CLI installer (idempotent) --------
  - path: /usr/local/bin/install-flux-cli.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/usr/bin/env bash
      set -euo pipefail
      if ! command -v flux >/dev/null 2>&1; then
        curl -s https://fluxcd.io/install.sh | bash
      fi

  # -------- MinIO setup (disk mount + service), safe if unset --------
  - path: /usr/local/bin/minio-setup.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/usr/bin/env bash
      set -euo pipefail
      source /etc/cloud/admin.env

      # Fast exit if not configured
      if [[ -z "${BACKUP_MOUNT:-}" || -z "${BACKUP_FSTYPE:-}" ]]; then
        echo "[minio] BACKUP_MOUNT/BACKUP_FSTYPE not set; skipping."
        exit 0
      fi

      mkdir -p "${BACKUP_MOUNT}"

      # If a UUID is provided, ensure it’s in fstab and mount
      if [[ -n "${BACKUP_UUID:-}" ]]; then
        if ! grep -q "${BACKUP_UUID}" /etc/fstab; then
          echo "UUID=${BACKUP_UUID} ${BACKUP_MOUNT} ${BACKUP_FSTYPE} noatime,nofail 0 2" >> /etc/fstab
        fi
        mount -a || true
      else
        echo "[minio] No BACKUP_UUID yet; will install binaries but not enable service."
      fi

      # --- choose correct binaries for host arch ---
      case "$(uname -m)" in
        x86_64|amd64) MINIO_ARCH=linux-amd64 ;;
        aarch64|arm64) MINIO_ARCH=linux-arm64 ;;
        *) echo "[minio] unsupported arch: $(uname -m)"; exit 0 ;;
      esac

      # Helper for reliable downloads
      dl() { curl --fail --location -fsS --retry 3 --connect-timeout 10 --max-time 120 -o "$1" "$2"; }

      # Install server + client if missing (or wrong arch)
      need_minio=true
      if command -v minio >/dev/null 2>&1; then
        # quick arch sanity: file(1) shows the target arch
        if file "$(command -v minio)" | grep -qiE '(aarch64|arm64|x86-64)'; then
          need_minio=false
        fi
      fi
      if $need_minio; then
        echo "[minio] installing server for ${MINIO_ARCH}"
        dl /usr/local/bin/minio "https://dl.min.io/server/minio/release/${MINIO_ARCH}/minio"
        chmod +x /usr/local/bin/minio
      fi

      need_mc=true
      if command -v mc >/dev/null 2>&1; then
        if file "$(command -v mc)" | grep -qiE '(aarch64|arm64|x86-64)'; then
          need_mc=false
        fi
      fi
      if $need_mc; then
        echo "[minio] installing client for ${MINIO_ARCH}"
        dl /usr/local/bin/mc "https://dl.min.io/client/mc/release/${MINIO_ARCH}/mc"
        chmod +x /usr/local/bin/mc
      fi

      # Env + unit
      mkdir -p /etc/minio
      cat >/etc/minio/minio.env <<EOF
      MINIO_ROOT_USER=${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
      MINIO_VOLUMES=${BACKUP_MOUNT}/minio-data
      MINIO_SERVER_URL=http://${MINIO_HOSTNAME}:${MINIO_PORT}
      MINIO_BROWSER_REDIRECT_URL=http://${MINIO_HOSTNAME}:${MINIO_CONSOLE_PORT}
      EOF
      chmod 0640 /etc/minio/minio.env

      cat >/etc/systemd/system/minio.service <<'EOF'
      [Unit]
      Description=MinIO Object Storage
      Wants=network-online.target
      After=network-online.target
      RequiresMountsFor=/mnt/backup

      [Service]
      EnvironmentFile=/etc/minio/minio.env
      ExecStart=/usr/local/bin/minio server $MINIO_VOLUMES --console-address ":9001" --address ":9000"
      Restart=always
      LimitNOFILE=65536

      [Install]
      WantedBy=multi-user.target
      EOF

      systemctl daemon-reload

      # Only start/enable if the mount exists
      if mountpoint -q "${BACKUP_MOUNT}"; then
        mkdir -p "${BACKUP_MOUNT}/minio-data"
        systemctl enable --now minio
        # health check with timeout
        for i in {1..30}; do
          if curl -fsS --connect-timeout 2 "http://127.0.0.1:${MINIO_PORT}/minio/health/ready" >/dev/null; then break; fi
          sleep 2
        done
        mc alias set local "http://127.0.0.1:${MINIO_PORT}" "${MINIO_ROOT_USER}" "${MINIO_ROOT_PASSWORD}" || true
        mc mb -p "local/${MINIO_BUCKET_LONGHORN}" || true
        mc mb -p "local/${MINIO_BUCKET_VMS}" || true
        echo "[minio] running with data at ${BACKUP_MOUNT}/minio-data"
      else
        echo "[minio] backup mount not present; service not enabled. Re-run after setting BACKUP_UUID and attaching disk."
      fi

  # -------- GitOps repo skeleton (Flux-friendly, idempotent) --------
  - path: /usr/local/bin/gitops-skeleton.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/usr/bin/env bash
      set -euo pipefail
      source /etc/cloud/admin.env

      [[ -z "${GITOPS_DIR:-}" ]] && { echo "[gitops] GITOPS_DIR not set; skipping."; exit 0; }
      [[ -z "${CLUSTER_NAME:-}" ]] && { echo "[gitops] CLUSTER_NAME not set; skipping."; exit 0; }

      # Non-interactive GitHub host key
      mkdir -p ~/.ssh
      ssh-keyscan github.com >> ~/.ssh/known_hosts 2>/dev/null || true
      chmod 600 ~/.ssh/known_hosts || true

      # Prefer the bootstrap key if it exists
      if [[ -s ~/.ssh/flux-bootstrap ]]; then
        { grep -q "Host github.com" ~/.ssh/config 2>/dev/null || cat <<'EOF' >> ~/.ssh/config
      Host github.com
        User git
        IdentityFile ~/.ssh/flux-bootstrap
        IdentitiesOnly yes
      EOF
        } && chmod 600 ~/.ssh/config
      fi

      # Ensure git identity (local)
      mkdir -p "${GITOPS_DIR}"
      cd "${GITOPS_DIR}"
      if [[ ! -d .git ]]; then
        if [[ -n "${GIT_REMOTE:-}" ]]; then
          echo "[gitops] cloning ${GIT_REMOTE} into ${GITOPS_DIR}"
          if git clone --branch "${GIT_BRANCH:-main}" "${GIT_REMOTE}" "${GITOPS_DIR}"; then
            :
          else
            echo "[gitops] clone failed (no deploy key yet?). Initializing local repo."
            git init -b "${GIT_BRANCH:-main}"
          fi
        else
          git init -b "${GIT_BRANCH:-main}"
        fi
      fi

      # Set local identity if provided
      if [[ -n "${ADMIN_GIT_NAME:-}" ]];  then git config user.name  "${ADMIN_GIT_NAME}";  fi
      if [[ -n "${ADMIN_GIT_EMAIL:-}" ]]; then git config user.email "${ADMIN_GIT_EMAIL}"; fi

      # Ensure remote configured
      if [[ -n "${GIT_REMOTE:-}" ]]; then
        if git remote get-url origin >/dev/null 2>&1; then
          git remote set-url origin "${GIT_REMOTE}" || true
        else
          git remote add origin "${GIT_REMOTE}" || true
        fi
      fi

      # Create tree
      mkdir -p "clusters/${CLUSTER_NAME}" sources/helm infra "config/${CLUSTER_NAME}" policies/gatekeeper/{templates,constraints} apps

      [[ -s .gitignore ]] || echo ".DS_Store" > .gitignore
      [[ -s README.md  ]] || cat > README.md <<EOF
      # GitOps (${CLUSTER_NAME})
      - clusters/${CLUSTER_NAME}: Flux entrypoint
      - sources: Helm/Git sources
      - infra: platform stack (ingress, cert-manager, longhorn, metallb, monitoring)
      - config/${CLUSTER_NAME}: cluster-specific config (issuers, IP pools, StorageClasses, values)
      - policies/gatekeeper: ConstraintTemplates + Constraints
      - apps: application stacks
      EOF

      # Flux entrypoint
      [[ -s "clusters/${CLUSTER_NAME}/kustomization.yaml" ]] || cat > "clusters/${CLUSTER_NAME}/kustomization.yaml" <<'EOF'
      apiVersion: kustomize.config.k8s.io/v1beta1
      kind: Kustomization
      resources:
        - sources.yaml
        - infra.yaml
        - config.yaml
        - policies.yaml
        - apps.yaml
      EOF

      [[ -s "clusters/${CLUSTER_NAME}/sources.yaml" ]] || cat > "clusters/${CLUSTER_NAME}/sources.yaml" <<'EOF'
      apiVersion: kustomize.toolkit.fluxcd.io/v1
      kind: Kustomization
      metadata: { name: sources, namespace: flux-system }
      spec:
        interval: 10m
        path: ./sources
        prune: true
        sourceRef: { kind: GitRepository, name: flux-system }
      EOF

      [[ -s "clusters/${CLUSTER_NAME}/infra.yaml" ]] || cat > "clusters/${CLUSTER_NAME}/infra.yaml" <<'EOF'
      apiVersion: kustomize.toolkit.fluxcd.io/v1
      kind: Kustomization
      metadata: { name: infra, namespace: flux-system }
      spec:
        interval: 5m
        path: ./infra
        prune: true
        sourceRef: { kind: GitRepository, name: flux-system }
        dependsOn: [ { name: sources } ]
      EOF

      [[ -s "clusters/${CLUSTER_NAME}/config.yaml" ]] || cat > "clusters/${CLUSTER_NAME}/config.yaml" <<EOF
      apiVersion: kustomize.toolkit.fluxcd.io/v1
      kind: Kustomization
      metadata: { name: ${CLUSTER_NAME}-config, namespace: flux-system }
      spec:
        interval: 10m
        path: ./config/${CLUSTER_NAME}
        prune: true
        sourceRef: { kind: GitRepository, name: flux-system }
        dependsOn: [ { name: infra } ]
      EOF

      [[ -s "clusters/${CLUSTER_NAME}/policies.yaml" ]] || cat > "clusters/${CLUSTER_NAME}/policies.yaml" <<'EOF'
      apiVersion: kustomize.toolkit.fluxcd.io/v1
      kind: Kustomization
      metadata: { name: policies, namespace: flux-system }
      spec:
        interval: 10m
        path: ./policies
        prune: true
        sourceRef: { kind: GitRepository, name: flux-system }
        dependsOn: [ { name: infra } ]
      EOF

      [[ -s "clusters/${CLUSTER_NAME}/apps.yaml" ]] || cat > "clusters/${CLUSTER_NAME}/apps.yaml" <<'EOF'
      apiVersion: kustomize.toolkit.fluxcd.io/v1
      kind: Kustomization
      metadata: { name: apps, namespace: flux-system }
      spec:
        interval: 5m
        path: ./apps
        prune: true
        sourceRef: { kind: GitRepository, name: flux-system }
        dependsOn:
          - name: infra
          - name: policies
          - name: '"'"'${CLUSTER_NAME}'"'"'-config
      EOF

      # HelmRepositories
      [[ -s sources/helm/jetstack.yaml ]] || cat > sources/helm/jetstack.yaml <<'EOF'
      apiVersion: source.toolkit.fluxcd.io/v1
      kind: HelmRepository
      metadata: { name: jetstack, namespace: flux-system }
      spec: { interval: 30m, url: https://charts.jetstack.io }
      EOF
      [[ -s sources/helm/ingress-nginx.yaml ]] || cat > sources/helm/ingress-nginx.yaml <<'EOF'
      apiVersion: source.toolkit.fluxcd.io/v1
      kind: HelmRepository
      metadata: { name: ingress-nginx, namespace: flux-system }
      spec: { interval: 30m, url: https://kubernetes.github.io/ingress-nginx }
      EOF
      [[ -s sources/helm/metallb.yaml ]] || cat > sources/helm/metallb.yaml <<'EOF'
      apiVersion: source.toolkit.fluxcd.io/v1
      kind: HelmRepository
      metadata: { name: metallb, namespace: flux-system }
      spec: { interval: 30m, url: https://metallb.github.io/metallb }
      EOF
      [[ -s sources/helm/longhorn.yaml ]] || cat > sources/helm/longhorn.yaml <<'EOF'
      apiVersion: source.toolkit.fluxcd.io/v1
      kind: HelmRepository
      metadata: { name: longhorn, namespace: flux-system }
      spec: { interval: 30m, url: https://charts.longhorn.io }
      EOF

      # Gatekeeper policies (same as before) ...
      [[ -s policies/gatekeeper/templates/disallow-latest.yaml ]] || cat > policies/gatekeeper/templates/disallow-latest.yaml <<'EOF'
      apiVersion: templates.gatekeeper.sh/v1
      kind: ConstraintTemplate
      metadata: { name: k8sdisallowlatest }
      spec:
        crd: { spec: { names: { kind: K8sDisallowLatest } } }
        targets:
        - target: admission.k8s.gatekeeper.sh
          rego: |
            package k8sdisallowlatest
            violation[{"msg": msg}] {
              input.review.kind.kind == "Pod"
              some c
              endswith(lower(input.review.object.spec.containers[c].image), ":latest")
              msg := sprintf("container %v uses :latest tag", [input.review.object.spec.containers[c].name])
            }
      EOF
      [[ -s policies/gatekeeper/constraints/disallow-latest.yaml ]] || cat > policies/gatekeeper/constraints/disallow-latest.yaml <<'EOF'
      apiVersion: constraints.gatekeeper.sh/v1beta1
      kind: K8sDisallowLatest
      metadata: { name: disallow-latest-images }
      spec:
        match: { kinds: [ { apiGroups: [""], kinds: ["Pod"] } ] }
      EOF

      [[ -s policies/gatekeeper/templates/required-resources.yaml ]] || cat > policies/gatekeeper/templates/required-resources.yaml <<'EOF'
      apiVersion: templates.gatekeeper.sh/v1
      kind: ConstraintTemplate
      metadata: { name: k8srequiredresources }
      spec:
        crd: { spec: { names: { kind: K8sRequiredResources } } }
        targets:
        - target: admission.k8s.gatekeeper.sh
          rego: |
            package k8srequiredresources
            has_all(r) { r.requests.cpu; r.requests.memory; r.limits.cpu; r.limits.memory }
            violation[{"msg": msg}] {
              input.review.kind.kind == "Pod"
              some i
              not has_all(input.review.object.spec.containers[i].resources)
              msg := sprintf("container %v must set cpu/memory requests and limits", [input.review.object.spec.containers[i].name])
            }
      EOF
      [[ -s policies/gatekeeper/constraints/required-resources.yaml ]] || cat > policies/gatekeeper/constraints/required-resources.yaml <<'EOF'
      apiVersion: constraints.gatekeeper.sh/v1beta1
      kind: K8sRequiredResources
      metadata: { name: require-requests-limits }
      spec:
        match: { kinds: [ { apiGroups: [""], kinds: ["Pod"] } ] }
      EOF

      [[ -s policies/gatekeeper/templates/arch-selector.yaml ]] || cat > policies/gatekeeper/templates/arch-selector.yaml <<'EOF'
      apiVersion: templates.gatekeeper.sh/v1
      kind: ConstraintTemplate
      metadata: { name: k8sarchselector }
      spec:
        crd: { spec: { names: { kind: K8sArchSelector } } }
        targets:
        - target: admission.k8s.gatekeeper.sh
          rego: |
            package k8sarchselector
            violation[{"msg": msg}] {
              input.review.kind.kind == "Pod"
              input.review.object.metadata.labels["arch"] == "amd64"
              not input.review.object.spec.nodeSelector["kubernetes.io/arch"]
              msg := "pod labeled arch=amd64 must set nodeSelector kubernetes.io/arch=amd64"
            }
            violation[{"msg": msg}] {
              input.review.kind.kind == "Pod"
              input.review.object.metadata.labels["arch"] == "amd64"
              input.review.object.spec.nodeSelector["kubernetes.io/arch"] != "amd64"
              msg := "pod labeled arch=amd64 must target kubernetes.io/arch=amd64"
            }
      EOF
      [[ -s policies/gatekeeper/constraints/arch-selector.yaml ]] || cat > policies/gatekeeper/constraints/arch-selector.yaml <<'EOF'
      apiVersion: constraints.gatekeeper.sh/v1beta1
      kind: K8sArchSelector
      metadata: { name: enforce-amd64-selector-when-labeled }
      spec:
        match: { kinds: [ { apiGroups: [""], kinds: ["Pod"] } ] }
      EOF

      git add -A
      git commit -m "bootstrap: Flux skeleton for ${CLUSTER_NAME}, sources, Gatekeeper" || true
      echo "[gitops] skeleton ready at ${GITOPS_DIR}"

  # -------- Flux deploy key generator (safe if user missing) --------
  - path: /usr/local/bin/flux-generate-key.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/usr/bin/env bash
      set -euo pipefail
      source /etc/cloud/admin.env
      USERNAME="${ADMIN_USER:-$SUDO_USER}"
      HOME_DIR="$(getent passwd "$USERNAME" | cut -d: -f6 || echo "/root")"
      mkdir -p "$HOME_DIR/.ssh"
      KEYFILE="$HOME_DIR/.ssh/flux-bootstrap"

      if [[ ! -f "${KEYFILE}" ]]; then
        sudo -u "$USERNAME" ssh-keygen -t ed25519 -C "flux-$(hostname)-$(date +%Y%m%d)" -N "" -f "${KEYFILE}"
        chown -R "$USERNAME":"$USERNAME" "$HOME_DIR/.ssh"
        echo "=== Flux deploy pubkey (add as repo deploy key, RW) ==="
        sudo -u "$USERNAME" cat "${KEYFILE}.pub"
      else
        echo "[flux] key exists at ${KEYFILE}"
      fi

  # -------- Talos config generator --------
  
  - path: /usr/local/bin/talos-generate-configs.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Load env
      source /etc/cloud/admin.env

      : "${CLUSTER_NAME:?Set in /etc/cloud/admin.env}"
      : "${VIP_API_IP:?Set in /etc/cloud/admin.env}"
      : "${TALOS_OUT_DIR:=/srv/talos}"

      ENDPOINT="https://${VIP_API_IP}:6443"
      mkdir -p "$TALOS_OUT_DIR"

      # Subnet prefix length (default /24 unless overridden in admin.env)
      : "${SUBNET_PREFIX:=24}"

      # DNS config (optional)
      DNS_LIST=()
      [[ -n "${DNS1:-}" ]] && DNS_LIST+=("\"${DNS1}\"")
      [[ -n "${DNS2:-}" ]] && DNS_LIST+=("\"${DNS2}\"")
      DNS_JSON=$(IFS=,; echo "${DNS_LIST[*]:-}")

      # Respect force regen
      regen_needed="no"
      if [[ "${TALOS_FORCE_REGEN:-no}" == "yes" ]]; then
        regen_needed="yes"
      elif [[ ! -f "$TALOS_OUT_DIR/controlplane.yaml" || ! -f "$TALOS_OUT_DIR/worker.yaml" ]]; then
        regen_needed="yes"
      fi

      if [[ "$regen_needed" == "yes" ]]; then
        echo "[talos] generating base machine configs for ${CLUSTER_NAME} (${ENDPOINT})"

        if [[ -n "${TALOS_SECRETS_FILE:-}" ]]; then
          [[ -f "$TALOS_SECRETS_FILE" ]] || { echo "[talos] TALOS_SECRETS_FILE not found: $TALOS_SECRETS_FILE"; exit 1; }
          talosctl gen config "$CLUSTER_NAME" "$ENDPOINT" \
            --with-secrets "$TALOS_SECRETS_FILE" \
            --output "$TALOS_OUT_DIR"
        else
          SECRETS="$TALOS_OUT_DIR/secrets.yaml"
          [[ -f "$SECRETS" ]] || talosctl gen secrets --output-file "$SECRETS"
          talosctl gen config "$CLUSTER_NAME" "$ENDPOINT" \
            --with-secrets "$SECRETS" \
            --output "$TALOS_OUT_DIR"
        fi
      else
        echo "[talos] base configs already exist in $TALOS_OUT_DIR (skipping gen)"
      fi

      base_cp="$TALOS_OUT_DIR/controlplane.yaml"
      [[ -f "$base_cp" ]] || { echo "[talos] missing $base_cp"; exit 1; }

      # Helper: patch a base control-plane config into a per-node config
      mk_control_cfg() {
        local HOST="$1" IP="$2" IFACE="$3"
        local OUT="$TALOS_OUT_DIR/controlplane-${HOST}.yaml"

        # JSON6902 patch: hostname + static IP + default route + optional DNS + workloads on CPs + systemExtensions
        cat >"$TALOS_OUT_DIR/.patch-${HOST}.json" <<'JSON'
      [
        {"op":"add","path":"/machine/network/hostname","value":"__HOST__"},
        {"op":"add","path":"/machine/network/interfaces","value":[
          {
            "interface":"__IFACE__",
            "addresses":["__IP__/__PFX__"],
            "routes":[{"network":"0.0.0.0/0","gateway":"__GW__"}],
            "dhcp": false
          }
        ]},
        {"op":"add","path":"/cluster/allowSchedulingOnControlPlanes","value":true},
        {"op":"add","path":"/machine/systemExtensions","value":{
          "officialExtensions":[
            "ghcr.io/siderolabs/iscsi-tools",
            "ghcr.io/siderolabs/nfs-utils"
          ]
        }}
      ]
      JSON

        # Substitute placeholders
        sed -i \
          -e "s|__HOST__|${HOST}|g" \
          -e "s|__IFACE__|${IFACE}|g" \
          -e "s|__IP__|${IP}|g" \
          -e "s|__PFX__|${SUBNET_PREFIX}|g" \
          -e "s|__GW__|${GATEWAY}|g" \
          "$TALOS_OUT_DIR/.patch-${HOST}.json"

        # Apply optional DNS patch
        if [[ -n "$DNS_JSON" ]]; then
          cat >"$TALOS_OUT_DIR/.patch-${HOST}-dns.json" <<JSON
      [
        {"op":"add","path":"/machine/network/nameservers","value":[${DNS_JSON}]}
      ]
      JSON
          talosctl machineconfig patch "$base_cp" \
            --patch "@$TALOS_OUT_DIR/.patch-${HOST}.json" \
            --patch "@$TALOS_OUT_DIR/.patch-${HOST}-dns.json" \
            > "$OUT"
        else
          talosctl machineconfig patch "$base_cp" \
            --patch "@$TALOS_OUT_DIR/.patch-${HOST}.json" \
            > "$OUT"
        fi

        echo "  wrote $OUT"
      }

      echo "[talos] creating per-node control-plane configs"

      # CONTROL VM
      : "${CONTROL_VM_HOSTNAME:?}"
      : "${CONTROL_VM_IP:?}"
      : "${CONTROL_VM_IFACE:?}"
      mk_control_cfg "$CONTROL_VM_HOSTNAME" "$CONTROL_VM_IP" "$CONTROL_VM_IFACE"

      # NUC1
      : "${NUC1_HOSTNAME:?}"
      : "${NUC1_IP:?}"
      : "${NUC1_IFACE:?}"
      mk_control_cfg "$NUC1_HOSTNAME" "$NUC1_IP" "$NUC1_IFACE"

      # NUC2
      : "${NUC2_HOSTNAME:?}"
      : "${NUC2_IP:?}"
      : "${NUC2_IFACE:?}"
      mk_control_cfg "$NUC2_HOSTNAME" "$NUC2_IP" "$NUC2_IFACE"

      # Cleanup temp patches
      rm -f "$TALOS_OUT_DIR"/.patch-*.json

      echo "[talos] done."

  # -------- Matchbox Configuration Publish --------
  
  - path: /usr/local/bin/matchbox-publish-configs.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/usr/bin/env bash
      set -Eeuo pipefail

      # Load admin config
      if [[ -f /etc/cloud/admin.env ]]; then
        # shellcheck disable=SC1091
        . /etc/cloud/admin.env
      else
        echo "[matchbox] /etc/cloud/admin.env missing; aborting" >&2
        exit 1
      fi

      : "${MATCHBOX_DIR:=/srv/matchbox}"
      : "${MATCHBOX_HTTP_PORT:=8080}"
      : "${TALOS_OUT_DIR:=/srv/talos}"

      # Required vars
      : "${ADMIN_VM_IP:?Set ADMIN_VM_IP}"
      : "${TALOS_VERSION:?Set TALOS_VERSION (e.g. v1.11.1)}"

      : "${CONTROL_VM_MAC:?Set CONTROL_VM_MAC}"
      : "${NUC1_MAC:?Set NUC1_MAC}"
      : "${NUC2_MAC:?Set NUC2_MAC}"

      : "${CONTROL_VM_HOSTNAME:?Set CONTROL_VM_HOSTNAME}"
      : "${NUC1_HOSTNAME:?Set NUC1_HOSTNAME}"
      : "${NUC2_HOSTNAME:?Set NUC2_HOSTNAME}"

      profiles_dir="$MATCHBOX_DIR/profiles"
      groups_dir="$MATCHBOX_DIR/groups"
      configs_dir="$MATCHBOX_DIR/configs"

      upper(){ tr '[:lower:]' '[:upper:]'; }

      install -d -m 0755 "$profiles_dir" "$groups_dir" "$configs_dir"

      # 0) Copy/refresh the per-node configs into matchbox/configs
      for H in "$CONTROL_VM_HOSTNAME" "$NUC1_HOSTNAME" "$NUC2_HOSTNAME"; do
        src="$TALOS_OUT_DIR/controlplane-${H}.yaml"
        dst="$configs_dir/controlplane-${H}.yaml"
        [[ -f "$src" ]] || { echo "[matchbox] missing $src — run talos-generate-configs.sh first"; exit 1; }
        install -m 0644 "$src" "$dst"
      done

      # 1) Control-plane profile (reusable)
      cat > "$profiles_dir/talos-controlplane.json" <<JSON
      {
        "id": "talos-controlplane",
        "name": "Talos Control Plane",
        "boot": {
          "kernel": "/assets/talos/${TALOS_VERSION}/vmlinuz",
          "initrd": ["/assets/talos/${TALOS_VERSION}/initramfs.xz"],
          "args": [
            "talos.platform=metal",
            "console=tty0",
            "console=ttyS0,115200n8",
            "ip=dhcp",
            "talos.config=http://{{.http_ip}}:${MATCHBOX_HTTP_PORT}/configs/{{.config}}"
          ]
        }
      }
      JSON

      # 2) Per-node groups (selectors by MAC, payload chooses per-node config)
      cvm_mac_uc="$(printf '%s' "$CONTROL_VM_MAC" | upper)"
      nuc1_mac_uc="$(printf '%s' "$NUC1_MAC" | upper)"
      nuc2_mac_uc="$(printf '%s' "$NUC2_MAC" | upper)"

      cat > "$groups_dir/${CONTROL_VM_HOSTNAME}.json" <<JSON
      {
        "id": "${CONTROL_VM_HOSTNAME}",
        "name": "${CONTROL_VM_HOSTNAME} (control-plane)",
        "profile": "talos-controlplane",
        "selector": { "mac": "${cvm_mac_uc}" },
        "metadata": { "http_ip": "${ADMIN_VM_IP}", "config": "controlplane-${CONTROL_VM_HOSTNAME}.yaml" }
      }
      JSON

      cat > "$groups_dir/${NUC1_HOSTNAME}.json" <<JSON
      {
        "id": "${NUC1_HOSTNAME}",
        "name": "${NUC1_HOSTNAME} (control-plane)",
        "profile": "talos-controlplane",
        "selector": { "mac": "${nuc1_mac_uc}" },
        "metadata": { "http_ip": "${ADMIN_VM_IP}", "config": "controlplane-${NUC1_HOSTNAME}.yaml" }
      }
      JSON

      cat > "$groups_dir/${NUC2_HOSTNAME}.json" <<JSON
      {
        "id": "${NUC2_HOSTNAME}",
        "name": "${NUC2_HOSTNAME} (control-plane)",
        "profile": "talos-controlplane",
        "selector": { "mac": "${nuc2_mac_uc}" },
        "metadata": { "http_ip": "${ADMIN_VM_IP}", "config": "controlplane-${NUC2_HOSTNAME}.yaml" }
      }
      JSON

      echo "[matchbox] wrote:"
      echo "  $profiles_dir/talos-controlplane.json"
      echo "  $groups_dir/${CONTROL_VM_HOSTNAME}.json"
      echo "  $groups_dir/${NUC1_HOSTNAME}.json"
      echo "  $groups_dir/${NUC2_HOSTNAME}.json"
      echo "  $configs_dir/controlplane-*.yaml"

      # Optional: nudge matchbox (it hot-reads JSON anyway)
      if command -v systemctl >/dev/null 2>&1 && systemctl is-active --quiet matchbox 2>/dev/null; then
        systemctl reload-or-restart matchbox || true
      fi

  # -------- convenience wrapper: run all steps safely anytime --------
  - path: /usr/local/bin/cloud-apply
    permissions: "0755"
    owner: root:root
    content: |
      #!/usr/bin/env bash
      set -Eeuo pipefail
      export DEBIAN_FRONTEND=noninteractive

      if [[ $EUID -ne 0 ]]; then
        exec sudo -E "$0" "$@"
      fi

      # pretty trace on error
      trap 'rc=$?; echo "[ERROR] line $LINENO: ${BASH_COMMAND} (exit $rc)"; exit $rc' ERR

      run() {
        echo
        echo "=== $(date -Is) :: $*"
        echo
        timeout --foreground "${TIMEOUT:-600s}" "$@"
      }

      echo "[apply] sourcing /etc/cloud/admin.env"
      source /etc/cloud/admin.env

      run /usr/local/sbin/setup-pxe.sh
      run /usr/local/bin/install-flux-cli.sh
      run /usr/local/bin/minio-setup.sh
      run /usr/local/bin/sops-setup.sh
      run /usr/local/bin/gitops-skeleton.sh
      run /usr/local/bin/flux-generate-key.sh
      run /usr/local/bin/talos-generate-configs.sh
      run /usr/local/bin/matchbox-publish-configs.sh

      # after
      set -a
      source /etc/cloud/admin.env
      set +a

      echo "[apply] done."

  # -------- Setup and install SOPs --------
  - path: /usr/local/bin/sops-setup.sh
    permissions: "0755"
    owner: root:root
    content: |
      #!/usr/bin/env bash
      set -Eeuo pipefail
      export DEBIAN_FRONTEND=noninteractive

      : "${GITOPS_DIR:=/srv/git/dozzly-network-home-cluster}"
      : "${SOPS_KEYS_DIR:=$HOME/.config/sops/age}"
      : "${SOPS_KEYS_FILE:=$SOPS_KEYS_DIR/keys.txt}"
      : "${SOPS_RECIPIENT:=}"
      : "${WRITE_SOPS_YAML:=yes}"        # yes/no
      : "${CREATE_FLUX_SECRET:=no}"      # yes/no (requires K8s up)
      : "${FLUX_NAMESPACE:=flux-system}"
      : "${APT_INSTALL:=yes}"
      : "${SKIP_SOPS_GEN:=0}"            # 1 = skip key gen and .sops.yaml changes entirely

      # --- put near the top, after your defaults ---
      : "${SOPS_KEYS_DIR_SYSTEM:=/etc/sops/age}"
      : "${SOPS_KEYS_FILE_SYSTEM:=${SOPS_KEYS_DIR_SYSTEM}/keys.txt}"

      # keep old env vars for compatibility; default them to the system path
      : "${SOPS_KEYS_DIR:=$SOPS_KEYS_DIR_SYSTEM}"
      : "${SOPS_KEYS_FILE:=$SOPS_KEYS_FILE_SYSTEM}"

      log(){ echo "[$(date -Is)] $*"; }
      have(){ command -v "$1" >/dev/null 2>&1; }

      repo_owner() {
        # Prefer the non-root user that invoked sudo, else the dir owner, else current user
        if [[ -n "${SUDO_USER:-}" && "${SUDO_USER}" != "root" ]]; then
          printf '%s' "$SUDO_USER"; return
        fi
        # owner of $GITOPS_DIR (numeric uid) → map to username
        local uid
        uid="$(stat -c '%u' "$GITOPS_DIR" 2>/dev/null || echo "")"
        if [[ -n "$uid" ]]; then
          getent passwd "$uid" | cut -d: -f1 | awk 'NF{print; exit}'; return
        fi
        id -un
      }

      repo_group() {
        local user; user="$(repo_owner)"
        # primary group of the owner; fallback to owner name
        id -gn "$user" 2>/dev/null || echo "$user"
      }

      install_age() {
        if have age; then return; fi
        if [[ "$APT_INSTALL" == "yes" ]]; then
          log "Installing 'age' via apt…"
          apt-get update -y
          apt-get install -y age || { log "Failed to install 'age'"; exit 1; }
        else
          log "age not found and APT_INSTALL=no; please install manually."; exit 1
        fi
      }

      install_sops() {
        if have sops; then return; fi
        if have snap; then
          log "Installing 'sops' via snap…"
          snap install sops --classic && return || log "snap install failed; trying GitHub binary…"
        fi
        log "Installing 'sops' from GitHub (static binary)…"
        case "$(uname -m)" in
          x86_64|amd64) arch="amd64" ;;
          aarch64|arm64) arch="arm64" ;;
          *) log "Unsupported arch: $(uname -m)"; exit 1 ;;
        esac
        latest_tag="$(curl -fsSLI -o /dev/null -w '%{url_effective}' https://github.com/mozilla/sops/releases/latest \
                       | sed -E 's@.*/tag/(v[0-9.]+).*@\1@')"
        [[ -n "$latest_tag" ]] || { log "Could not determine latest sops tag"; exit 1; }
        url="https://github.com/mozilla/sops/releases/download/${latest_tag}/sops-${latest_tag}.linux.${arch}"
        curl -fsSL "$url" -o /usr/local/bin/sops || { log "Download failed: $url"; exit 1; }
        chmod +x /usr/local/bin/sops
        sops --version
      }

      ensure_keys() {
        mkdir -p "$SOPS_KEYS_DIR_SYSTEM"
        chmod 700 "$SOPS_KEYS_DIR_SYSTEM"

        # Prevent concurrent clobbering
        local lock="${SOPS_KEYS_FILE_SYSTEM}.lock"
        exec 9>"$lock"
        if ! flock -w 15 9; then
          log "Could not obtain lock on $lock"; exit 1
        fi

        # Cleanup trap must tolerate set -u
        local tmpdir=""; local tmpfile=""
        trap '[[ -n "${tmpdir:-}" ]] && rm -rf "$tmpdir"' RETURN

        if [[ ! -f "$SOPS_KEYS_FILE_SYSTEM" ]] || ! grep -q 'AGE-SECRET-KEY-' "$SOPS_KEYS_FILE_SYSTEM"; then
          log "Generating age keypair at $SOPS_KEYS_FILE_SYSTEM"
          umask 077
          tmpdir="$(mktemp -d)"
          tmpfile="$tmpdir/keys.txt"
          age-keygen -o "$tmpfile"
          install -m 600 "$tmpfile" "$SOPS_KEYS_FILE_SYSTEM"
        else
          log "age key exists at $SOPS_KEYS_FILE_SYSTEM (keeping)"
        fi

        chmod 600 "$SOPS_KEYS_FILE_SYSTEM"

        # Optional: symlink per-user default to the system key
        mkdir -p "$HOME/.config/sops/age"
        ln -sf "$SOPS_KEYS_FILE_SYSTEM" "$HOME/.config/sops/age/keys.txt" || true
      }

      get_recipient() {
        # Stronger and format-proof: derive from private key
        # age-keygen -y prints the public key (recipient) for the given identity
        if [[ -n "$SOPS_RECIPIENT" ]]; then
          echo "$SOPS_RECIPIENT"; return
        fi
        age-keygen -y "$SOPS_KEYS_FILE_SYSTEM" 2>/dev/null | awk 'NF{print; exit}'
      }

      write_sops_yaml() {
        local recip file tmp owner group
        recip="$(get_recipient)"
        [[ -n "$recip" ]] || { log "ERROR: could not determine age recipient"; exit 1; }

        file="$GITOPS_DIR/.sops.yaml"
        mkdir -p "$GITOPS_DIR"

        owner="$(repo_owner)"
        group="$(repo_group)"

        # We use scalar 'age: "RECIPIENT"' because some sops builds expect a string, not a YAML sequence.
        create_fresh() {
          install -o "$owner" -g "$group" -m 0644 /dev/stdin "$file" <<YAML
      # Auto-generated by sops-setup.sh. Edit carefully.
      creation_rules:
        - path_regex: '(.*/)?secret(\.sops)?\.(ya?ml)$'
          encrypted_regex: '^(data|stringData)$'
          age: '${recip}'
        - path_regex: '(^|.*/)secrets/.*\.(ya?ml)$'
          encrypted_regex: '^(data|stringData)$'
          age: '${recip}'
      YAML
          log "Created $file with recipient ${recip} (owner ${owner}:${group})"
        }

        if [[ ! -f "$file" ]]; then
          create_fresh
          return
        fi

        # If file is malformed (duplicate 'age:' keys or list syntax), just rewrite it cleanly.
        if grep -qE 'age:\s*\[' "$file"; then
          log "$file uses list syntax for age; rewriting as scalar"
          create_fresh
          return
        fi
        if awk 'p&&/^ *age:/{c++} /^creation_rules:/{p=1} END{exit !(c>2)}' "$file"; then
          log "$file seems to have duplicate age keys; rewriting"
          create_fresh
          return
        fi

        # If our recipient already present → nothing to do
        if grep -q -F "age: '${recip}'" "$file"; then
          log "$file already contains recipient; leaving unchanged"
        else
          # Update any existing 'age:' lines to our recipient (first two rules)
          tmp="$(mktemp)"
          awk -v R="${recip}" '
            BEGIN{changed=0}
            /^ *age: / && changed<2 { sub(/^ *age:.*/, "    age: '\''" R "'\''"); changed++ }
            { print }
          ' "$file" > "$tmp" && mv "$tmp" "$file"
          chown "$owner:$group" "$file" || true
          chmod 0644 "$file" || true
          log "Updated recipient in $file to ${recip} (owner ${owner}:${group})"
        fi
      }

      create_flux_secret() {
        if ! have kubectl; then log "kubectl not found; skipping Flux sops-age secret"; return; fi
        if ! kubectl version --short >/dev/null 2>&1; then log "Kubernetes not reachable; skipping Flux sops-age secret"; return; fi
        # Always use the system key file
        kubectl -n "$FLUX_NAMESPACE" create secret generic sops-age \
          --from-file=age.agekey="$SOPS_KEYS_FILE_SYSTEM" \
          --dry-run=client -o yaml | kubectl apply -f -
        log "Ensured Secret ${FLUX_NAMESPACE}/sops-age"
      }

      main() {
        if [[ "$SKIP_SOPS_GEN" == "1" ]]; then
          log "SKIP_SOPS_GEN=1 set; skipping key gen and .sops.yaml changes"
          exit 0
        fi
        install_age
        install_sops
        ensure_keys
        [[ "$WRITE_SOPS_YAML" == "yes" ]] && write_sops_yaml || log "WRITE_SOPS_YAML=no; skipped .sops.yaml"
        [[ "$CREATE_FLUX_SECRET" == "yes" ]] && create_flux_secret || log "CREATE_FLUX_SECRET=no; skipped Flux secret"
        log "Done. Public age recipient:"
        awk '/^# public key:/{print $0}' "$SOPS_KEYS_FILE_SYSTEM"
      }
      main "$@"

# --- 4) Commands (ordered; binaries & dirs before starting services) ---
runcmd:
  # Apply Netplan Settings
  - netplan generate
  - netplan apply
  # Admin bootstrap (idempotent)
  - [ bash, -lc, "/usr/local/sbin/admin-setup.sh || true" ]
  # Prepare directories and service user ownership
  - [ bash, -lc, "install -d -m 0755 /srv/matchbox/{assets,profiles,groups,configs,ignition} /var/lib/tftpboot /home/${admin_user}/.bashrc.d" ]
  - [ bash, -lc, "chown -R matchbox:matchbox /srv/matchbox" ]
  - [ bash, -lc, "chown -R ${admin_user}:${admin_user} /home/${admin_user}/.bashrc.d" ]
  # optional: useful tools
  - [ bash, -lc, "apt-get install -y git jq htop nfs-common" ]
  # Detect architecture → /etc/admin-server-arch.env
  - |
    bash -lc '
      set -eu
      case "$(uname -m)" in
        aarch64|arm64) GOARCH=arm64 ;;
        x86_64|amd64)  GOARCH=amd64 ;;
        *)             GOARCH=arm64 ;;
      esac
      echo "GOARCH=$GOARCH" > /etc/admin-server-arch.env
    '

  # Install matchbox binary (ARM64/AMD64 aware)
  - |
    bash -lc '
      set -euo pipefail
      if [ -f /etc/admin-server-arch.env ]; then . /etc/admin-server-arch.env; fi
      : "${GOARCH:=$(uname -m | sed -e s/aarch64/arm64/ -e s/x86_64/amd64/)}"
      V="v0.11.0"
      T="$(mktemp -d /tmp/matchbox.XXXXXX)"
      curl -fsSL -o "$T/matchbox.tar.gz" \
        "https://github.com/poseidon/matchbox/releases/download/${V}/matchbox-${V}-linux-${GOARCH}.tar.gz"
      tar -xzf "$T/matchbox.tar.gz" -C "$T"
      install -m 0755 "$T/matchbox-${V}-linux-${GOARCH}/matchbox" /usr/local/bin/matchbox
    '

  # kubectl (latest stable) and talosctl
  - |
    bash -lc '
      set -eu
      . /etc/admin-server-arch.env
      V="$(curl -fsSL https://dl.k8s.io/release/stable.txt)"
      curl -fsSL -o /usr/local/bin/kubectl "https://dl.k8s.io/release/${V}/bin/linux/${GOARCH}/kubectl"
      chmod +x /usr/local/bin/kubectl
      TV="v1.11.1"
      curl -fsSL -o /usr/local/bin/talosctl "https://github.com/siderolabs/talos/releases/download/${TV}/talosctl-linux-${GOARCH}"
      chmod +x /usr/local/bin/talosctl
    '

  # Enable & start services (after everything exists)

  - [ bash, -lc, 'rm -f /usr/sbin/policy-rc.d || true' ]

  - |
    bash -lc '
      set -euo pipefail
      IFACE=$(sed -n "s/^interface=//p" /etc/dnsmasq.d/00-iface.conf)
      for i in $(seq 1 30); do
        ip -4 addr show dev "$IFACE" | grep -q "inet " && break
        sleep 1
      done
    '

  - [ bash, -lc, "systemctl daemon-reload" ]
  - [ bash, -lc, "systemctl enable --now dnsmasq-iface" ]
  - [ bash, -lc, "dnsmasq --test" ]
  - [ bash, -lc, "systemctl enable --now matchbox caddy tftpd-hpa" ]

  # Helm & Krew
  - [ bash, -lc, "/usr/local/bin/helm-install.sh || true" ]
  - [ bash, -lc, "sudo -u ${admin_user} bash -lc '/usr/local/bin/krew-install.sh' || true" ]

  # Firewall (optional)
  - [ bash, -lc, "ufw allow OpenSSH; ufw allow http; ufw allow 69/udp; ufw allow 67/udp; ufw allow 6443/tcp; ufw --force enable" ]

  # Ensure .bashrc loads .bashrc.d snippets
  - [ bash, -lc, 'grep -q bashrc.d "/home/${admin_user}/.bashrc" 2>/dev/null || printf "%s\n" "# Load per-snippet bashrc.d files if present" "if [ -d \"$HOME/.bashrc.d\" ]; then" "  for f in \"$HOME/.bashrc.d/\"*.sh; do [ -r \"$f\" ] && . \"$f\"; done" "fi" >> "/home/${admin_user}/.bashrc"' ]
  - [ bash, -lc, 'chown ${admin_user}:${admin_user} "/home/${admin_user}/.bashrc" || true' ]

  # Install Flux & Notify
  - [ bash, -c, "/usr/local/bin/install-flux-cli.sh" ]
  - [ bash, -c, "echo 'First boot complete. Edit /etc/cloud/admin.env then run: sudo cloud-apply'" ]

final_message: |
  Admin server ready 🚀

  Next steps:
   1. Edit /etc/cloud/admin.env to set cluster name, Git repo, and IPs.
   1a. Run the Skeleton Script and own the directory.
        sudo /usr/local/bin/gitops-skeleton.sh
        sudo chown -R jdoswell:jdoswell /srv/
   2. Generate a Flux SSH key (if not already):
        ssh-keygen -t ed25519 -C "flux-dozzly-cluster" -N "" -f ~/.ssh/flux-bootstrap
        cat ~/.ssh/flux-bootstrap.pub
      → Add this key as a Deploy key (with write access) on GitHub repo jackchild/dozzly-network-home-cluster.
   3. Allow the VM to use that key for GitHub:
        ssh-keyscan github.com >> ~/.ssh/known_hosts
        echo -e "Host github.com\n  User git\n  IdentityFile ~/.ssh/flux-bootstrap\n  IdentitiesOnly yes" >> ~/.ssh/config
   4. Test GitHub SSH auth:
        ssh -T git@github.com
   5. Initialize/push GitOps skeleton:
        cd /srv/git/dozzly-cluster-gitops
        git remote add origin git@github.com:dozzly/dozzly-network-home-cluster.git || git remote set-url origin git@github.com:dozzly/dozzly-network-home-cluster.git
        git branch -M main
        git push -u origin main
   6. Generate Talos Keys
        talosctl kubeconfig --nodes <CONTROL_PLANE_IP> --force
        export KUBECONFIG=$PWD/kubeconfig
   7. When first NUC is up and Talos cluster is bootstrapped:
        flux bootstrap github \
          --owner dozzly \
          --repository dozzly-network-home-cluster \
          --branch main \
          --path clusters/dozzly-cluster \
          --private-key-file ~/.ssh/flux-bootstrap

  Notes:
   - MinIO will only start once BACKUP_UUID is set & disk is attached.
   - Talos configs are under /srv/talos (regenerated via talos-generate-configs.sh).
   - GitOps skeleton lives under /srv/git/dozzly-cluster-gitops.
   - Re-run "sudo cloud-apply" after editing /etc/cloud/admin.env.
   - kubectl commands:
   - set admin to arm64 and opt-in only
   - kubectl taint node atherton role=admin:NoSchedule
   - kubectl label node atherton node-role.do/arm-admin=true kubernetes.io/arch=arm64 --overwrite
   - kubectl label node goldstone node-role.do/amd-work=true kubernetes.io/arch=amd64 --overwrite
   - kubectl label node gee       node-role.do/amd-work=true kubernetes.io/arch=amd64 --overwrite
final_message: |
  Admin server ready 🚀

  Next steps:
   1. Edit /etc/cloud/admin.env to set cluster name, Git repo, and IPs.
   2. sudo chown -R jdoswell:jdoswell /srv/
   3. Generate a Flux SSH key (if not already):
        ssh-keygen -t ed25519 -C "flux-dozzly-cluster" -N "" -f ~/.ssh/flux-bootstrap
        cat ~/.ssh/flux-bootstrap.pub
      → Add this key as a Deploy key (with write access) on GitHub repo jackchild/dozzly-network-home-cluster.
   4. Allow the VM to use that key for GitHub:
        ssh-keyscan github.com >> ~/.ssh/known_hosts
        echo -e "Host github.com\n  User git\n  IdentityFile ~/.ssh/flux-bootstrap\n  IdentitiesOnly yes" >> ~/.ssh/config
   5. Test GitHub SSH auth:
        ssh -T git@github.com
   6. Initialize/push GitOps skeleton:
        mkdir -p /srv/git/
        cd /srv/git/
        git clone https://github.com/dozzly/dozzly-network-home-cluster.git
   7. When first NUC is up and Talos cluster is bootstrapped:
        flux bootstrap github \
          --owner dozzly \
          --repository dozzly-network-home-cluster \
          --branch main \
          --path clusters/dozzly-cluster \
          --private-key-file ~/.ssh/flux-bootstrap

  Notes:
   - MinIO will only start once BACKUP_UUID is set & disk is attached.
   - Talos configs are under /srv/talos (regenerated via talos-generate-configs.sh).
   - GitOps skeleton lives under /srv/git/dozzly-network-home-cluster.
   - Re-run "sudo cloud-apply" after editing /etc/cloud/admin.env.


